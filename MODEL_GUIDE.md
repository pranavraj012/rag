# ü§ñ Language Model Guide for SOP Knowledge Assistant

## Overview
This guide helps you choose the best language model for your system based on GPU capabilities and quality requirements.

## üéØ Recommended Models (GTX 1650 4GB VRAM)

### 1. **Microsoft Phi-2** ‚≠ê RECOMMENDED
```python
LLM_MODEL=microsoft/phi-2
```
- **Parameters:** 2.7B
- **VRAM Usage:** ~2.5GB
- **Quality:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent
- **Speed:** ‚ö°‚ö°‚ö°‚ö° Fast
- **Best For:** High-quality Q&A, complex reasoning, detailed explanations
- **Pros:** 
  - Exceptional quality for size
  - Trained by Microsoft Research
  - Great instruction following
  - Fast inference
- **Cons:** 
  - Slightly higher VRAM usage
  - Requires `trust_remote_code=True`

### 2. **TinyLlama-1.1B-Chat** üí® FASTEST
```python
LLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
```
- **Parameters:** 1.1B
- **VRAM Usage:** ~1.2GB
- **Quality:** ‚≠ê‚≠ê‚≠ê‚≠ê Very Good
- **Speed:** ‚ö°‚ö°‚≠ê‚ö°‚ö° Very Fast
- **Best For:** Quick responses, chat-style interactions, light systems
- **Pros:**
  - Lowest VRAM usage
  - Fastest generation speed
  - Chat-optimized
  - Leaves room for other apps
- **Cons:**
  - Slightly less detailed than Phi-2
  - May need more guidance

### 3. **StableLM-2-1.6B** ‚öñÔ∏è BALANCED
```python
LLM_MODEL=stabilityai/stablelm-2-1_6b
```
- **Parameters:** 1.6B
- **VRAM Usage:** ~1.8GB
- **Quality:** ‚≠ê‚≠ê‚≠ê‚≠ê Excellent
- **Speed:** ‚ö°‚ö°‚ö°‚ö° Fast
- **Best For:** Balanced performance and quality
- **Pros:**
  - Great quality-to-size ratio
  - Stability AI's latest architecture
  - Good instruction following
  - Reliable performance
- **Cons:**
  - Middle ground (not best at anything specific)
  - Requires `trust_remote_code=True`

### 4. **DialoGPT-Medium** ü™∂ LIGHTWEIGHT
```python
LLM_MODEL=microsoft/DialoGPT-medium
```
- **Parameters:** 350M (0.35B)
- **VRAM Usage:** ~400MB
- **Quality:** ‚≠ê‚≠ê‚≠ê Good
- **Speed:** ‚ö°‚ö°‚ö°‚ö°‚ö° Fastest
- **Best For:** Extreme low memory, CPU fallback, older GPUs
- **Pros:**
  - Minimal VRAM usage
  - Proven stability
  - Fast loading
  - Good CPU performance
- **Cons:**
  - Lower quality responses
  - Less detailed explanations
  - May struggle with complex queries

---

## üìä Quick Comparison Table

| Model | Params | VRAM | Quality | Speed | Best Use Case |
|-------|--------|------|---------|-------|--------------|
| **Phi-2** | 2.7B | 2.5GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Production quality |
| **TinyLlama** | 1.1B | 1.2GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | Fast responses |
| **StableLM-2** | 1.6B | 1.8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Balanced performance |
| **DialoGPT** | 0.35B | 0.4GB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | Legacy/Low memory |

---

## üéÆ GPU-Specific Recommendations

### GTX 1650 (4GB VRAM) - Your System ‚úÖ
**Recommended:** `microsoft/phi-2`
- Perfect fit for 4GB VRAM
- Leaves ~1.5GB for other tasks
- Best quality available for your GPU

**Alternative:** `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
- If you need maximum speed
- If running other GPU applications simultaneously
- More memory headroom

### RTX 4060 (8GB VRAM) üöÄ
**Recommended:** `microsoft/phi-2`
- Same model, but can handle larger batches
- Can run with other GPU apps easily
- Consider larger models like:
  - `mistralai/Mistral-7B-Instruct-v0.2` (7B, ~4GB)
  - `meta-llama/Llama-2-7b-chat-hf` (7B, ~4GB)

### CPU Only / Low Memory Systems üíª
**Recommended:** `microsoft/DialoGPT-medium`
- Reasonable CPU performance
- Lowest memory footprint
- Proven reliability

---

## üîÑ How to Change Models

### Method 1: Edit `.env` file
```bash
# Open .env file and uncomment your choice:
LLM_MODEL=microsoft/phi-2
```

### Method 2: Programmatic change in code
```python
# In query_engine.py initialization
query_engine = QueryEngine(model_name="microsoft/phi-2")
```

### Method 3: Environment variable
```bash
# Windows PowerShell
$env:LLM_MODEL="microsoft/phi-2"

# Linux/Mac
export LLM_MODEL="microsoft/phi-2"
```

---

## üìà Expected Improvements with Phi-2

Upgrading from DialoGPT-medium to Phi-2:

### Answer Quality
- **Detail Level:** 3-5x more comprehensive
- **Accuracy:** 20-30% improvement
- **Coherence:** Significantly better structured responses
- **Context Understanding:** Much better at following instructions

### Performance
- **Loading Time:** ~8-10 seconds (vs ~5 seconds for DialoGPT)
- **Query Time:** ~3-5 seconds (vs ~2-3 seconds for DialoGPT)
- **VRAM Usage:** 2.5GB (vs 0.4GB for DialoGPT)

### Real-World Impact
**Example Query:** "How is a defect list prepared for annual repair?"

**DialoGPT-medium Output (~50 words):**
```
The defect list is prepared by crew. Deck side by Master, engine side by driver...
[truncated, incomplete]
```

**Phi-2 Output (~150-200 words):**
```
For conducting an annual repair of a vessel, a comprehensive defect/flaws list 
must be prepared through the following systematic process:

1. Initial Preparation:
   - The vessel's crew initiates the defect list preparation
   - Two separate lists are created for deck and engine sides

2. Deck Side Defect List:
   - Prepared by the Master (Captain)
   - Covers all deck machineries and equipment
   - Includes wheelhouse and superstructure components
   - Documents condition for ensuring good working order

3. Engine Side Defect List:
   - Compiled by the Chief Engineer (driver)
   - Encompasses all engine room machinery and equipment
   - Covers pump room, compressor compartment, and cooler compartment
   - Details all mechanical defects and required repairs

4. Verification Process:
   - Lists are examined by T.A/A.D. (Marine) onboard
   - Sent to surveyor for professional assessment
   - Main repairs are identified and prioritized
   
This systematic approach ensures comprehensive documentation before repair work begins.
```

---

## üõ†Ô∏è Troubleshooting

### Model Won't Load
1. **Check VRAM:** Monitor GPU memory usage
2. **Try smaller model:** Switch to TinyLlama or DialoGPT
3. **Update drivers:** Ensure latest NVIDIA drivers
4. **Clear cache:** Delete `~/.cache/huggingface/`

### Out of Memory Error
```python
# Add to query_engine.py before model loading:
import torch
torch.cuda.empty_cache()
```

### Slow Performance
1. Ensure GPU is being used (check logs)
2. Close other GPU applications
3. Consider TinyLlama for faster responses
4. Reduce `max_length` in pipeline config

---

## üéØ Final Recommendation

**For Your GTX 1650 4GB System:**

Start with **Phi-2** for best quality:
```bash
LLM_MODEL=microsoft/phi-2
```

If you need more speed or memory:
```bash
LLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
```

Both are **MUCH better** than DialoGPT-medium and will work great on your GPU!

---

## üìö Additional Resources

- [Hugging Face Model Hub](https://huggingface.co/models)
- [Phi-2 Model Card](https://huggingface.co/microsoft/phi-2)
- [TinyLlama Model Card](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- [GPU Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)
