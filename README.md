# SOP Knowledge Assistant

A powerful AI-powered document Q&A system for Standard Operating Procedures (SOPs) built with Streamlit, LangChain, and ChromaDB.

## ğŸš€ Features

- **Document Processing**: Supports PDF, DOCX, and TXT files
- **GPU Acceleration**: Optimized for NVIDIA GPUs (GTX 1650, RTX 4060, etc.)
- **Vector Search**: Semantic search using ChromaDB
- **AI Q&A**: Natural language question answering using DialoGPT
- **Web Interface**: User-friendly Streamlit interface

## ğŸ“‹ Requirements

- Python 3.8+
- NVIDIA GPU with CUDA support (recommended)
- 2GB+ GPU VRAM (4GB+ recommended)

## ğŸ› ï¸ Installation

1. **Clone the repository:**
```bash
git clone <your-repo-url>
cd sop_project
```

2. **Create virtual environment:**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Install PyTorch with CUDA support:**
```bash
# For CUDA 11.8 (GTX 1650, RTX 30/40 series)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1 (Latest RTX 40 series)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## ğŸš€ Usage

1. **Start the application:**
```bash
streamlit run app.py
```

2. **Open your browser:** http://localhost:8501

3. **Upload documents** using the sidebar

4. **Ask questions** about your SOPs!

## ğŸ’¾ Model

The system uses **TinyLlama-1.1B-Chat** (1.1B parameters) for efficient answer generation:

- **Size:** 1.1B parameters (~1.5GB VRAM)
- **Quality:** â­â­â­â­ Very Good
- **Optimized for:** GTX 1650 4GB and lower-end GPUs
- **Features:** Q&A, Step-by-step instructions, detailed explanations
- **Advantage:** Smaller footprint, faster responses

### First-Time Setup

On first run, TinyLlama will be downloaded (~2.2GB, one-time):
- Download time: 2-5 minutes (depending on connection)
- Cached at: `~/.cache/huggingface/hub/`
- Loading time: 3-5 seconds (subsequent runs)

## ğŸ–¥ï¸ GPU Support

### Tested Configurations:
- **GTX 1650 (4GB)**: âœ… Works perfectly with Phi-2
- **RTX 4060 (8GB)**: âœ… Excellent performance
- **RTX 3070/4070+**: âœ… Optimal performance

### Performance:
- **Model Loading**: ~3 seconds on GPU
- **Query Processing**: ~2-4 seconds per query
- **VRAM Usage**: ~1.5GB (TinyLlama)
- **Leaves room**: ~2.5GB free for other applications

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ document_processor.py  # Document text extraction and chunking
â”œâ”€â”€ vector_store.py        # ChromaDB vector database
â”œâ”€â”€ query_engine.py        # AI query processing
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env                  # Configuration file (GTX 1650 optimized)
â”œâ”€â”€ .env.rtx4060          # Configuration for RTX 4060 users
â”œâ”€â”€ documents/           # Place your SOP documents here
â”œâ”€â”€ chroma_db/          # Vector database storage (auto-created)
â””â”€â”€ tests/              # Test files and utilities
```

## âš™ï¸ Configuration

Edit `.env` file to customize:
```bash
DOCUMENTS_FOLDER=./documents
CHROMA_PATH=./chroma_db
COLLECTION_NAME=sop-knowledge
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

## ğŸ§ª Testing

Run the quick test to verify everything works:
```bash
python tests/test_phi2.py
```

Or run comprehensive tests:
```bash
python tests/test_advanced.py
python tests/final_test.py
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

MIT License - Feel free to use for personal and commercial projects!

## ğŸ†˜ Troubleshooting

### CUDA Issues:
```bash
# Check CUDA availability
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

### Memory Issues:
- Reduce `CHUNK_SIZE` in .env
- Close other GPU applications
- Use CPU fallback if needed

### Model Download Issues:
- Check internet connection
- Manually clear cache: `rm -rf ~/.cache/huggingface/`

### Duplicate Documents in Search Results:
If you're getting the same content multiple times:
```bash
# Check for duplicates
python check_vector_store.py

# Rebuild vector store (removes all duplicates)
python rebuild_vector_store.py
```

**Note:** Always use `rebuild_vector_store.py` when re-uploading documents to avoid duplicates.

### Low Confidence Scores:
- Ensure no duplicate chunks (use `rebuild_vector_store.py`)
- Check if query matches document content well
- Confidence is color-coded: ğŸŸ¢ High (â‰¥75%), ğŸŸ¡ Medium (50-74%), ğŸ”´ Low (<50%)